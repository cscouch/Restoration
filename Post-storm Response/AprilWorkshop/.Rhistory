mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~PERCENT, ~ISLAND + ANALYSIS_YEAR, design = subset(repdes, GROUP=="CORAL"), svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = PERCENT, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = PERCENT - se, ymax = PERCENT + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(PROP ~ ISLAND * ANALYSIS_YEAR, design = subset(des, GROUP=="CORAL"))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(PROP ~ ISLAND * ANALYSIS_YEAR, design = subset(des, GROUP=="CORAL"))
library(dplyr)
library(tidyr)
library(ggplot2)
library(survey)
options(dplyr.summarise.inform = FALSE)
# read in data
cover = read.csv("C:/Users/courtney.s.couch/Documents/GitHub/December-2023-StRS-Stats-Workshop/Data/BenthicCover_SITE_analysisready.csv")
# prepare data for analysis (filtered to Main Hawaiian Islands)
mhi = cover %>% rename(SECTOR = PooledSector_Viztool) %>% #change name of column to "SECTOR"
distinct(SITEVISITID, SITE, .keep_all = TRUE) %>% #filter duplicates
mutate(NH = ifelse(is.na(NH) & DEPTH_BIN =="Mid", 50, #manually specify a NH value for one of the strata that didn't have NH
ifelse(is.na(NH),25, NH))) %>%
group_by(ANALYSIS_YEAR, SECTOR, SEC_NAME, STRATA) %>% mutate(NH = NH / n()) %>% #calculate NH (total possible sites) by year, sector, sec name and strata
group_by(ANALYSIS_YEAR, SECTOR, STRATA) %>% mutate(NH = sum(NH), n = n()) %>% #add up NHs and n (sites) for sectors that were pooled
filter(REGION == "MHI") %>% #remove strata with only 1 site and just include MHI sites
mutate(sw = NH / n, #Calculate survey weight
STRAT_CONC = paste(ANALYSIS_YEAR, REGION, ISLAND, SECTOR, STRATA, sep = "_"), #create new column with concatenated strata variable
ANALYSIS_YEAR = as.character(ANALYSIS_YEAR), #change to character
SECTOR = as.character(SECTOR), #change to character
STRATA = as.character(STRATA)) %>% #change to character
mutate(across(c(CORAL, CCA, MA, TURF), ~./100)) %>% # convert percent variables to proportions so we can use binomial
# remove islands with incomplete years
group_by(ISLAND) %>% filter(n_distinct(ANALYSIS_YEAR) == 4) %>% # only include islands that have 4 survey years
ungroup() #converts to standard dataframe
#options(survey.lonely.psu = "remove")
options(survey.lonely.psu = "adjust")
## define survey design
des = svydesign(id = ~1, strata = ~STRAT_CONC, weights = ~sw, data = mhi)
## replicate-weights survey design
# repdes = as.svrepdesign(des)
## calculate island/year means and SE
# standard design
isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = des, svymean)
# replicate-weights design
# isl_yearmean = svyby(~CORAL, ~ISLAND + ANALYSIS_YEAR, design = repdes, svymean)
# plot weighted mean coral cover by island and year
ggplot(isl_yearmean, aes(x = ANALYSIS_YEAR, y = CORAL, fill = ANALYSIS_YEAR)) +
geom_bar(stat = "identity", position = position_dodge2(preserve = "single"), width = 1, color = "black") +
geom_errorbar(aes(ymin = CORAL - se, ymax = CORAL + se), width = 0.2) +
facet_wrap(~ISLAND, nrow = 1) +
guides(fill = "none") +
theme_bw() +
theme(panel.grid.minor = element_blank(),
panel.grid.major = element_blank(),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
axis.text.x = element_text(angle = -90, hjust = 0))
# fit Gaussian model
mod.1 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR, design = des)
summary(mod.1)
#Test for significance of your fixed effects
car::Anova(mod.1, type = 3, test.statistic = "F")
# fit binomial model
mod.2 = svyglm(CORAL ~ ISLAND + ANALYSIS_YEAR, design = des, family = "binomial") # you will get an warning because it's expecting a 0/1, but it's ok the math will still work out
mod.3 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR, design = des, family = "binomial")
# compare models (ignore eff.p and deltabar- telling you how AIC was adjusted)
AIC(mod.2, mod.3)
#guidance on AIC is that you should be using a combination of significance of predictors/likelihood ratio tests and AIC, use AIC carefully by itself
car::Anova(mod.1, type = 3, test.statistic = "F")
# ------------------------------------------------------------------------------------------------------------
# residual diagnostics
# Residuals tells you the difference between what the value was vs. what the model told you it was
# standardize by dividing by SD so you can identify certain points that were of concern
#resids = svydiags::svystdres(mod.3, stvar = "STRAT_CONC")$stdresids
resids = scale(mod.3$residuals)
resids = (mod.3$residuals - mean(mod.3$residuals))/sd(mod.3$residuals)
#Should we be concerned about this plot?
#In standard normal model, residuals should fall withing 2SD of mean
#Kyle, explain why we we should plot fitted residuals rather than use straight up plot(resids)
#When working with survey design don't use plot(mod.3)
plot(fitted(mod.3), resids)
hist(resids)
#
# VIF of min and max depth
1 / (1 - with(mhi, cor(new_MIN_DEPTH_M, new_MAX_DEPTH_M, use = "pairwise.complete.obs"))^2)
# highly correlated; would only include one in model
# plot max depth vs. proportion coral cover
ggplot(mhi, aes(x = new_MAX_DEPTH_M, y = CORAL)) + geom_point() + geom_smooth() + theme_bw()
# no evidence of non-linear trend, but let's add it to the model
# add max depth as linear term
mod.3.1 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + new_MAX_DEPTH_M, design = des, family = "binomial")
summary(mod.3.1)
# add max depth as 2nd degree polynomial
mod.3.2 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + poly(new_MAX_DEPTH_M, 2, raw = TRUE), design = des, family = "binomial")
# compare by AIC
AIC(mod.3.1, mod.3.2)
summary(mod.3.2)
# evidence of a non-linear trend in presence of other covariates that was not apparent in simple 2D plot
# increase to a 3rd degree polynomial
mod.3.3 = svyglm(CORAL ~ ISLAND * ANALYSIS_YEAR + poly(new_MAX_DEPTH_M, 3, raw = TRUE), design = des, family = "binomial")
# compare by AIC
AIC(mod.3.2, mod.3.3)
# no evidence of improved fit with 3rd degree-- we settle on 2nd degree polynomial
# summarize
anova(mod.3.2)
car::Anova(mod.3.2, type = 3, test.statistic = "F")
t(sapply(attr(mod.3.2$terms, "term.labels"), function(x) regTermTest(mod.3.2, x, method = "WorkingWald")[c("df", "ddf", "p")]))
# pseudo R-squared
jtools::summ(mod.3.2)
# planned comparisons
#Testing 2019 vs. 2010-12 in Kauai
#need to identify the reference levels for each categorical list
contr = multcomp::glht(mod.3.2, linfct = c("ANALYSIS_YEAR2019 = 0",
"ANALYSIS_YEAR2019 + ISLANDMaui:ANALYSIS_YEAR2019 = 0",
"ANALYSIS_YEAR2019 + ISLANDKauai:ANALYSIS_YEAR2019 = 0"))
summary(contr)
round(100 * (exp(confint(contr)$confint) - 1), 1)
6.5-2.3
4.2/6.5
86/336
4*10
210*40
/10
8400/10
50*210
9*11
23/3
23/4
69/4
213814+168442+181204
23.72/13.99
23.72-13.99
9.73/13.99
9.73/23.72
rm(list=ls())
library(gdata)             # needed for drop_levels()
library(reshape)           # reshape library inclues the cast() function used below
library(RODBC)            # to connect to oracle
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp.R")
#This script combines all historical raw CPC and CoralNet annotations, generates analysis ready data,
#generates site-level % cover for all sites then generates strata and weighed means at the sector and island-level
#in v3, we've made several updates to the pooling scheme and added in the 2022 data.
#Note- CRED/CREP/ESD made the switch from CPC to CoralNet in 2015, but some of the legacy 2012 imagery was analyzed in CoralNet
rm(list=ls())
library(gdata)             # needed for drop_levels()
library(reshape)           # reshape library inclues the cast() function used below
library(RODBC)            # to connect to oracle
#LOAD LIBRARY FUNCTIONS ...
source("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Functions/Benthic_Functions_newApp_vTAOfork.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/core_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/fish_team_functions.R")
source("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/lib/Islandwide Mean&Variance Functions.R")
#Climate data - this is from CPCE
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_BIA_CLIMATE_PERM.rdata")   #bia
cli$SITE<-SiteNumLeadingZeros(cli$SITE)
#BIA data - this is from CPCE
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_BIA_STR_RAW_NEW.rdata")   #bia
bia$SITE<-SiteNumLeadingZeros(bia$SITE)
#CNET data - from CoralNet
#These data contain human annotated data. There may be a small subset of robot annotated data.
#The robot annotations are included because the confidence threshold in CoralNet was set to 70-90% allowing the robot to annotate points when it was 70-90% certain.
#2019 NWHI data not in these view because it was analyzed as part of a bleaching dataset
load("T:/Benthic/Data/REA Coral Demography & Cover/Raw from Oracle/ALL_CNET_Annotations.rdata") #load data
cnet<-select(cnet,-c("TYPE"))
head(cnet)
cnet$SITE<-as.factor(cnet$SITE)
cnet$SITE<-SiteNumLeadingZeros(cnet$SITE)
#Read in survey master and sector files
#sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER_w2013benthic.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER.csv")
#Read in survey master and sector files
#sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/Benthic-Scripts/Survey Master Prep/SURVEY_MASTER.csv")
sm<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/SURVEY MASTER.csv")
sectors<-read.csv("C:/Users/Courtney.S.Couch/Documents/GitHub/fish-paste/data/Sectors-Strata-Areas.csv")
#Temporary work around for merging in 2014-2017 NWHI data that hasn't been uploaded to Oracle yet- remove this once Michael has incorporated data
new.nw<-read.csv("T:/Benthic/Data/REA Coral Demography & Cover/Raw Data from CoralNet/2014-2017_NWHI_CnetAnnotations_formatted.csv")
new.nw<-new.nw %>% drop_na(ROUNDID) #remove blank rows
new.cnet<-new.nw
class(new.cnet$DATE_)
class(new.cnet$DATE_TAKEN)
#Date conversations still not working
new.cnet$DATE_<-lubridate::mdy(new.cnet$DATE_)
new.cnet$DATE_TAKEN<-lubridate::ymd(new.cnet$DATE_TAKEN);head(new.cnet$DATE_TAKEN)
new.cnet$DATE_ANNOTATED<-lubridate::ymd_hms(new.cnet$DATE_ANNOTATED);head(new.cnet$DATE_ANNOTATED)
#########PLACEHOLDER Temporary work around for merging in 2023 SWAINS data
new.nw<-read.csv("T:/Benthic/Data/REA Coral Demography & Cover/Raw Data from CoralNet/2014-2017_NWHI_CnetAnnotations_formatted.csv")
new.nw<-new.nw %>% drop_na(ROUNDID) #remove blank rows
new.cnet<-new.nw
class(new.cnet$DATE_)
class(new.cnet$DATE_TAKEN)
#Date conversations still not working
new.cnet$DATE_<-lubridate::mdy(new.cnet$DATE_)
new.cnet$DATE_TAKEN<-lubridate::ymd(new.cnet$DATE_TAKEN);head(new.cnet$DATE_TAKEN)
new.cnet$DATE_ANNOTATED<-lubridate::ymd_hms(new.cnet$DATE_ANNOTATED);head(new.cnet$DATE_ANNOTATED)
#combine old cnet and 2015 & 2017 nwhi cnet data
cnet<-rbind(cnet,new.cnet)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/Restoration/Post-storm Response/AprilWorkshop"))
library(dplyr)
library(sp)
library(sf)
library(raster)
library(ncf) # for gcdist()
library(ggsn)
library(ggspatial)
library(ggrepel)
library(lubridate)
library(hms)
library(tidyr)
### read in tracks
dmg <- read.csv("DMGScores-OAHU-041724.csv")
g1<-read.csv("DMG-OAHU-GROUP1-041724.csv")
g2<-read.csv("DMG-OAHU-GROUP2-4.17.2024.csv")
g3<-read.csv("DMG-OAHU-GROUP3-041724.csv")
g4<-read.csv("DMG-OAHU-GROUP4-041724.csv")
tracks<-rbind(g1,g2,g3,g4)
View(dmg)
debris<-subset(dmg,Damage_score =="D")
debris<-subset(dmg,Damage_Score =="D")
dmg<-subset(dmg,Damage_Score !="D")
g1<-read.csv("DMG-OAHU-GROUP1-041724.csv")
g2<-read.csv("DMG-OAHU-GROUP2-4.17.2024.csv")
g3<-read.csv("DMG-OAHU-GROUP3-041724.csv")
g4<-read.csv("DMG-OAHU-GROUP4-041724.csv")
tracks<-rbind(g1,g2,g3,g4)
#Clean up track file and convert date time columns
tracks <- tracks %>%
dplyr::select(Latitude, Longitude, time) %>%
separate(time,into=c("Date", "Time2"),
sep=" ", convert = TRUE, extra = "merge") %>%
mutate(date = ymd(Date),
datetime_utc = as_datetime(paste(Date, Time2)),
datetime_hst = as_datetime(datetime_utc, tz = 'US/Hawaii'))%>%
separate(datetime_hst,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(Latitude, Longitude,Date, Time)
tracks$Date<-as.Date(tracks$Date)
head(tracks)
tracks<-rbind(g1,g2,g3,g4)
#Clean up track file and convert date time columns
tracks <- tracks %>%
dplyr::select(Organization,Latitude, Longitude, time) %>%
separate(time,into=c("Date", "Time2"),
sep=" ", convert = TRUE, extra = "merge") %>%
mutate(date = ymd(Date),
datetime_utc = as_datetime(paste(Date, Time2)),
datetime_hst = as_datetime(datetime_utc, tz = 'US/Hawaii'))%>%
separate(datetime_hst,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(Latitude, Longitude,Date, Time)
tracks<-rbind(g1,g2,g3,g4)
#Clean up track file and convert date time columns
tracks <- tracks %>%
dplyr::select(Organization,Latitude, Longitude, time) %>%
separate(time,into=c("Date", "Time2"),
sep=" ", convert = TRUE, extra = "merge") %>%
mutate(date = ymd(Date),
datetime_utc = as_datetime(paste(Date, Time2)),
datetime_hst = as_datetime(datetime_utc, tz = 'US/Hawaii'))%>%
separate(datetime_hst,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(Organization,Latitude, Longitude,Date, Time)
head(tracks)
#Clean up track file and convert date time columns
dmg$Time <- as.POSIXct(dmg$Time,format="%H:%M:%S")
dmg <- dmg %>%
separate(Time,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(-c(Notes))
# Convert time strings to POSIXct datetime objects
tracks$Time <- as.POSIXct(paste("2024-04-17", tracks$Time), format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
dmg$Time <- as.POSIXct(paste("2024-04-17", dmg$Time), format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
start_time<-min(dmg$Time)
end_time<-max(dmg$Time)
#Trim tracks to start and end
tracks<-tracks %>%
filter(between(Time, start_time, end_time))
# Determine the max time in the track dataset
max_time_track <- max(tracks$Time)
max_time_track
tracks$Time
dmg <- read.csv("DMGScores-OAHU-041724.csv")
debris<-subset(dmg,Damage_Score =="D")
dmg<-subset(dmg,Damage_Score !="D")
g1<-read.csv("DMG-OAHU-GROUP1-041724.csv")
g2<-read.csv("DMG-OAHU-GROUP2-4.17.2024.csv")
g3<-read.csv("DMG-OAHU-GROUP3-041724.csv")
g4<-read.csv("DMG-OAHU-GROUP4-041724.csv")
tracks<-rbind(g1,g2,g3,g4)
#Clean up track file and convert date time columns
tracks <- tracks %>%
dplyr::select(Latitude, Longitude, time) %>%
separate(time,into=c("Date", "Time2"),
sep=" ", convert = TRUE, extra = "merge") %>%
mutate(date = ymd(Date),
datetime_utc = as_datetime(paste(Date, Time2)),
datetime_hst = as_datetime(datetime_utc, tz = 'US/Hawaii'))%>%
separate(datetime_hst,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(Latitude, Longitude,Date, Time)
tracks$Date<-as.Date(tracks$Date)
# #tracks$Date<-dplyr::filter(Date =="2024-04-17")
# date.range <- interval(as.POSIXct("9:43:00"),
#                        as.POSIXct("11:31:42"))
#
# tracks <- tracks[tracks$Time %within% date.range,]
#
# head(tracks)
#Clean up track file and convert date time columns
dmg$Time <- as.POSIXct(dmg$Time,format="%H:%M:%S")
dmg <- dmg %>%
separate(Time,into=c("d", "Time"),
sep=" ", convert = TRUE, extra = "merge")%>%
dplyr::select(-c(Notes))
# Convert time strings to POSIXct datetime objects
tracks$Time <- as.POSIXct(paste("2024-04-17", tracks$Time), format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
dmg$Time <- as.POSIXct(paste("2024-04-17", dmg$Time), format = "%Y-%m-%d %H:%M:%S", tz = "UTC")
start_time<-min(dmg$Time)
end_time<-max(dmg$Time)
#Trim tracks to start and end
tracks<-tracks %>%
filter(between(Time, start_time, end_time))
# Determine the max time in the track dataset
max_time_track <- max(tracks$Time)
max_time_track
# Set end times to one second before the next damage score starts, except for the last damage score
dmg <- dmg %>%
arrange(Time) %>%
mutate(End_Time = if_else(row_number() == n(), max_time_track, lead(Time) - seconds(1)))
dmg <- dmg %>%
arrange(Time) %>%
# mutate(End_Time = if_else(row_number() == n(), max_time_track, lead(Time) - seconds(1)))
mutate(End_Time = lead(Time) - seconds(1))
head(dmg)
# Create intervals
dmg$Interval <- interval(dmg$Time, dmg$End_Time)
# Initialize the Damage_Score column in track
tracks$Damage_Score <- NA
# Iterate over each row in track to assign Damage_Score
for (i in 1:nrow(tracks)) {
for (j in 1:nrow(dmg)) {
if (tracks$Time[i] %within% dmg$Interval[j]) {
tracks$Damage_Score[i] <- dmg$Damage_Score[j]
break  # Stop checking once the correct interval is found
}
}
}
dmg<-subset(dmg,Organziation=="Group 1")
dmg<-subset(dmg,Organization=="Group 1")
# Initialize the Damage_Score column in track
tracks$Damage_Score <- NA
# Iterate over each row in track to assign Damage_Score
for (i in 1:nrow(tracks)) {
for (j in 1:nrow(dmg)) {
if (tracks$Time[i] %within% dmg$Interval[j]) {
tracks$Damage_Score[i] <- dmg$Damage_Score[j]
break  # Stop checking once the correct interval is found
}
}
}
head(dmg)
View(dmg)
rm(list=ls())
dir = Sys.info()[7]
setwd(paste0("C:/Users/", dir, "/Documents/GitHub/Restoration/Post-storm Response/AprilWorkshop"))
library(dplyr)
library(sp)
library(sf)
library(raster)
library(ncf) # for gcdist()
library(ggsn)
library(ggspatial)
library(ggrepel)
library(lubridate)
library(hms)
library(tidyr)
### read in tracks
dmg <- read.csv("DMGScores-OAHU-041724.csv")
debris<-subset(dmg,Damage_Score =="D")
dmg<-subset(dmg,Damage_Score !="D")
View(dmg)
